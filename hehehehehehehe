import cv2
import torch
import torchvision
from torchvision import transforms
import numpy as np
from PIL import Image

# 1. GPU 설정
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 2. 모델 변경: DeepLabV3 -> LR-ASPP (속도 훨씬 빠름)
# torchvision 0.11 호환 (pretrained=True)
print("모델 로딩 중...")
model = torchvision.models.segmentation.lraspp_mobilenet_v3_large(pretrained=True)
model.to(device)
model.half() # FP16 적용
model.eval()
print("모델 로딩 완료")

# 3. 전처리 수정: Resize는 OpenCV에서 하므로 여기서는 뺍니다.
preprocess = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# 4. GStreamer 파이프라인 최적화
# appsink에 drop=1을 넣어 지연(Latency)을 줄입니다.
gst_pipeline = (
    "nvarguscamerasrc ! "
    "video/x-raw(memory:NVMM), width=1280, height=720, format=NV12, framerate=30/1 ! "
    "nvvidconv ! video/x-raw, format=BGRx ! videoconvert ! "
    "video/x-raw, format=BGR ! appsink drop=1" 
)

cap = cv2.VideoCapture(gst_pipeline, cv2.CAP_GSTREAMER)

if not cap.isOpened():
    print("카메라를 열 수 없습니다.")
    exit()

# 변수 설정
frame_skip = 3  # 15는 너무 깁니다. 3 정도로 줄이세요.
frame_count = 0
current_mask = None # 마스크 저장용

# 추론할 크기 (작을수록 빠름)
INFER_SIZE = (224, 224)

while True:
    ret, frame = cap.read()
    if not ret:
        continue

    # 원본 해상도 저장 (나중에 화면에 보여줄 때 사용)
    # 1280x720은 너무 크면 화면에 꽉 차니까, 보기 좋게 640x360 정도로 줄여서 표시합니다.
    display_frame = cv2.resize(frame, (640, 360)) 

    # 추론용 리사이즈 (224x224)
    frame_resized = cv2.resize(frame, INFER_SIZE)

    # 매 frame_skip 프레임마다 세그멘테이션 수행 (비동기 느낌)
    if frame_count % frame_skip == 0:
        img = Image.fromarray(cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB))
        # FP16 입력 변환
        input_tensor = preprocess(img).unsqueeze(0).to(device).half() 

        with torch.no_grad():
            output = model(input_tensor)['out'][0]
        
        pred = output.argmax(0).byte().cpu().numpy()
        
        # Human mask (클래스 15) -> 0 또는 255 값으로 변환
        mask_small = (pred == 15).astype(np.uint8) * 255
        
        # 마스크를 화면 크기(640x360)에 맞게 확대 (Nearest가 빠름)
        current_mask = cv2.resize(mask_small, (display_frame.shape[1], display_frame.shape[0]), interpolation=cv2.INTER_NEAREST)

    frame_count += 1

    # 합성 단계
    if current_mask is not None:
        # 마스크가 1채널이므로 3채널(RGB)로 변경
        mask_3ch = cv2.cvtColor(current_mask, cv2.COLOR_GRAY2BGR)
        
        # 비트 연산으로 원본에서 사람만 떼어내기 (배경 검정)
        result = cv2.bitwise_and(display_frame, mask_3ch)
    else:
        # 아직 추론 결과가 없으면 그냥 원본 출력
        result = display_frame

    cv2.imshow("Human Only", result)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
