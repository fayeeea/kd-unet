import cv2
import torch
import torchvision
from torchvision import transforms
import numpy as np
from PIL import Image
import time

# 1. GPU 설정
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 2. 모델 변경: DeepLabV3 -> LR-ASPP (훨씬 가벼움!)
# 0.11 버전 호환을 위해 pretrained=True 사용
print("모델 로딩 중...")
model = torchvision.models.segmentation.lraspp_mobilenet_v3_large(pretrained=True)
model.to(device)

# ★ 핵심 최적화: FP16 모드 (메모리 절반, 속도 향상)
model.half() 
model.eval()
print("모델 로딩 완료! (FP16 적용됨)")

# 3. 전처리 (Resize는 OpenCV에서 직접 하므로 여기서는 제거)
preprocess = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# 비디오 캡처
cap = cv2.VideoCapture(0)
# 카메라 해상도 설정 (640x480) - 원본은 크게 봅니다
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

# 추론할 크기 (작을수록 빠름, 224 or 256 추천)
INFER_SIZE = (224, 224)

prev_time = 0

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # FPS 계산을 위한 시간 측정
    curr_time = time.time()
    
    # [1] 추론을 위해 이미지 작게 줄이기
    frame_small = cv2.resize(frame, INFER_SIZE)
    
    # OpenCV(BGR) -> PIL(RGB)
    img = Image.fromarray(cv2.cvtColor(frame_small, cv2.COLOR_BGR2RGB))
    
    # [2] 텐서 변환 및 FP16 적용
    input_tensor = preprocess(img).unsqueeze(0).to(device)
    input_tensor = input_tensor.half()  # ★ 입력 데이터도 half로 변환해야 함

    # [3] 추론 (Inference)
    with torch.no_grad():
        output = model(input_tensor)['out'][0]
    
    # [4] 결과 처리
    pred = output.argmax(0).byte().cpu().numpy()
    
    # Human mask (클래스 15)
    mask_small = (pred == 15).astype(np.uint8) * 255 # 0 or 255로 변환

    # [5] 마스크를 다시 원본 크기(640x480)로 확대
    # INTER_NEAREST가 가장 빠름, 부드럽게 하려면 INTER_LINEAR 사용
    mask_original = cv2.resize(mask_small, (frame.shape[1], frame.shape[0]), interpolation=cv2.INTER_NEAREST)

    # [6] 배경 합성 (배경을 검은색으로)
    # mask가 3채널(RGB)이 아니므로 차원 확장 필요
    mask_3ch = cv2.cvtColor(mask_original, cv2.COLOR_GRAY2BGR)
    
    # 비트 연산으로 마스크 적용 (이미지가 커서 행렬 곱셈보다 비트 연산이 조금 더 효율적일 수 있음)
    result = cv2.bitwise_and(frame, mask_3ch)

    # FPS 출력
    fps = 1 / (curr_time - prev_time)
    prev_time = curr_time
    cv2.putText(result, f"FPS: {fps:.1f}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

    # 화면 표시
    cv2.imshow("Jetson Nano Segment", result)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
