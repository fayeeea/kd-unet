#터미널에서 이거 한줄씩 입력하슈
#sudo apt-get install libprotobuf-dev protobuf-compiler
#git clone https://github.com/NVIDIA-AI-IOT/torch2trt
#cd torch2trt
#sudo python3 setup.py install

import cv2
import torch
import torchvision
from torchvision import transforms
import numpy as np
from PIL import Image
import time
from collections import deque

# ★ 중요: torch2trt 라이브러리 임포트
from torch2trt import torch2trt

# GPU 설정
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print("1. 모델 로드 중...")
# MobileNetV3 기반 DeepLabV3 모델 로드
model = torchvision.models.segmentation.deeplabv3_mobilenet_v3_large(pretrained=True)
model.to(device).eval().half()

# 전처리
preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# ★ 중요: TensorRT 엔진으로 변환 (최초 1회 실행 시 시간이 좀 걸립니다)
print("2. TensorRT 변환 중... (약 1~3분 소요될 수 있음)")

# 변환을 위한 더미 데이터 생성 (입력 크기와 똑같아야 함)
# FP16 모드이므로 데이터도 half() 처리
dummy_input = torch.zeros((1, 3, 224, 224)).to(device).half()

# 변환 실행 (fp16_mode=True가 핵심)
model_trt = torch2trt(model, [dummy_input], fp16_mode=True)

print("3. 변환 완료! 추론 시작")

# 카메라 파이프라인
gst_pipeline = (
    "nvarguscamerasrc ! "
    "video/x-raw(memory:NVMM), width=1280, height=720, format=NV12, framerate=30/1 ! "
    "nvvidconv ! video/x-raw, format=BGRx ! videoconvert ! "
    "video/x-raw, format=BGR ! appsink drop=1"
)
cap = cv2.VideoCapture(gst_pipeline, cv2.CAP_GSTREAMER)
if not cap.isOpened():
    print("카메라를 열 수 없습니다.")
    exit()

frame_times = deque(maxlen=30)

while True:
    cap.grab()
    ret, frame = cap.retrieve()
    if not ret:
        continue

    start_time = time.time()

    frame_resized = cv2.resize(frame, (224, 224))
    img = Image.fromarray(cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB))
    
    # 입력 데이터 생성
    input_tensor = preprocess(img).unsqueeze(0).to(device).half()

    # ★ 변경점: 최적화된 model_trt 사용
    # torch2trt 모델은 딕셔너리가 아니라 텐서를 바로 반환하기도 하므로 확인 필요
    # 보통 Semantic Segmentation 모델은 변환 후 바로 Tensor를 뱉습니다.
    with torch.no_grad():
        output = model_trt(input_tensor)
    
    # TensorRT 변환 모델은 출력이 dict가 아닐 수 있음 (구조에 따라 다름)
    # 만약 에러가 나면 output['out'][0] 대신 그냥 output[0] 사용
    if isinstance(output, dict):
        output = output['out'][0]
    else:
        output = output[0] # 튜플이나 텐서로 나올 경우

    pred = output.argmax(0).byte().cpu().numpy()
    human_mask = (pred == 15).astype(np.uint8)
    
    # 결과 합성
    result = frame_resized * human_mask[:, :, np.newaxis]

    end_time = time.time()
    frame_times.append(end_time - start_time)
    
    # 0으로 나누기 방지
    if sum(frame_times) > 0:
        avg_fps = 1.0 / (sum(frame_times) / len(frame_times))
    else:
        avg_fps = 0

    cv2.putText(result, f"FPS: {avg_fps:.1f}", (10, 25),
                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

    cv2.imshow("TensorRT Optimized", result)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
